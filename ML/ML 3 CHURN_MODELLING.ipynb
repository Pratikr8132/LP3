{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9897ba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efea5d66",
   "metadata": {},
   "source": [
    "## **Loading the Dataset**\n",
    "First we load the dataset and find out the number of columns, rows, NULL values, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8724836",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('churn_modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5a4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b1f87e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec1361e",
   "metadata": {},
   "source": [
    "## **Cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34bb6bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3064950e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreditScore        0\n",
       "Geography          0\n",
       "Gender             0\n",
       "Age                0\n",
       "Tenure             0\n",
       "Balance            0\n",
       "NumOfProducts      0\n",
       "HasCrCard          0\n",
       "IsActiveMember     0\n",
       "EstimatedSalary    0\n",
       "Exited             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de021c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.00000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>650.528800</td>\n",
       "      <td>38.921800</td>\n",
       "      <td>5.012800</td>\n",
       "      <td>76485.889288</td>\n",
       "      <td>1.530200</td>\n",
       "      <td>0.70550</td>\n",
       "      <td>0.515100</td>\n",
       "      <td>100090.239881</td>\n",
       "      <td>0.203700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>96.653299</td>\n",
       "      <td>10.487806</td>\n",
       "      <td>2.892174</td>\n",
       "      <td>62397.405202</td>\n",
       "      <td>0.581654</td>\n",
       "      <td>0.45584</td>\n",
       "      <td>0.499797</td>\n",
       "      <td>57510.492818</td>\n",
       "      <td>0.402769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>350.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.580000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>584.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51002.110000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>652.000000</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>97198.540000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>100193.915000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>718.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>127644.240000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>149388.247500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>850.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>250898.090000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>199992.480000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        CreditScore           Age        Tenure        Balance  NumOfProducts  \\\n",
       "count  10000.000000  10000.000000  10000.000000   10000.000000   10000.000000   \n",
       "mean     650.528800     38.921800      5.012800   76485.889288       1.530200   \n",
       "std       96.653299     10.487806      2.892174   62397.405202       0.581654   \n",
       "min      350.000000     18.000000      0.000000       0.000000       1.000000   \n",
       "25%      584.000000     32.000000      3.000000       0.000000       1.000000   \n",
       "50%      652.000000     37.000000      5.000000   97198.540000       1.000000   \n",
       "75%      718.000000     44.000000      7.000000  127644.240000       2.000000   \n",
       "max      850.000000     92.000000     10.000000  250898.090000       4.000000   \n",
       "\n",
       "         HasCrCard  IsActiveMember  EstimatedSalary        Exited  \n",
       "count  10000.00000    10000.000000     10000.000000  10000.000000  \n",
       "mean       0.70550        0.515100    100090.239881      0.203700  \n",
       "std        0.45584        0.499797     57510.492818      0.402769  \n",
       "min        0.00000        0.000000        11.580000      0.000000  \n",
       "25%        0.00000        0.000000     51002.110000      0.000000  \n",
       "50%        1.00000        1.000000    100193.915000      0.000000  \n",
       "75%        1.00000        1.000000    149388.247500      0.000000  \n",
       "max        1.00000        1.000000    199992.480000      1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cfdae7",
   "metadata": {},
   "source": [
    "# **Separating the features and the labels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a39eacd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 10), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X=df.iloc[:, :df.shape[1]-1].values       #Independent Variables\n",
    "y=df.iloc[:, -1].values                   #Dependent Variable\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5247dd7",
   "metadata": {},
   "source": [
    "**Encoding categorical (string based) data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bd7a3e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France' 'Spain' 'France' 'France' 'Spain' 'Spain' 'France' 'Germany'] ... will now become: \n",
      "[0 2 0 0 2 2 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X[:8,1], '... will now become: ')\n",
    "\n",
    "label_X_country_encoder = LabelEncoder()\n",
    "X[:,1] = label_X_country_encoder.fit_transform(X[:,1])\n",
    "print(X[:8,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c121302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Female' 'Female' 'Female' 'Female' 'Female' 'Male'] ... will now become: \n",
      "[0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(X[:6,2], '... will now become: ')\n",
    "\n",
    "label_X_gender_encoder = LabelEncoder()\n",
    "X[:,2] = label_X_gender_encoder.fit_transform(X[:,2])\n",
    "print(X[:6,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7779970a",
   "metadata": {},
   "source": [
    "**Split the countries into respective dimensions. Converting the string features into their own dimensions.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00880886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 0.0, ..., 1, 1, 101348.88],\n",
       "       [0.0, 0.0, 1.0, ..., 0, 1, 112542.58],\n",
       "       [1.0, 0.0, 0.0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [1.0, 0.0, 0.0, ..., 0, 1, 42085.58],\n",
       "       [0.0, 1.0, 0.0, ..., 1, 0, 92888.52],\n",
       "       [1.0, 0.0, 0.0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = ColumnTransformer([(\"countries\", OneHotEncoder(), [1])], remainder=\"passthrough\") # 1 is the country column\n",
    "X = transform.fit_transform(X)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f959190",
   "metadata": {},
   "source": [
    "**Dimensionality reduction. A 0 on two countries means that the country has to be the one variable which wasn't included**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd494dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 11)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X[:,1:]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc517f5",
   "metadata": {},
   "source": [
    "# **Splitting the Dataset**\n",
    "Training and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78e2bccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81845166",
   "metadata": {},
   "source": [
    "**Normalize the train and test data**\n",
    "\n",
    "`['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7ea4d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=StandardScaler()\n",
    "X_train[:,np.array([2,4,5,6,7,10])] = sc.fit_transform(X_train[:,np.array([2,4,5,6,7,10])])\n",
    "X_test[:,np.array([2,4,5,6,7,10])] = sc.transform(X_test[:,np.array([2,4,5,6,7,10])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91d7443d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5698444 ,  1.74309049,  0.16958176, ...,  0.64259497,\n",
       "        -1.03227043,  1.10643166],\n",
       "       [ 1.75486502, -0.57369368, -2.30455945, ...,  0.64259497,\n",
       "         0.9687384 , -0.74866447],\n",
       "       [-0.5698444 , -0.57369368, -1.19119591, ...,  0.64259497,\n",
       "        -1.03227043,  1.48533467],\n",
       "       ...,\n",
       "       [-0.5698444 , -0.57369368,  0.9015152 , ...,  0.64259497,\n",
       "        -1.03227043,  1.41231994],\n",
       "       [-0.5698444 ,  1.74309049, -0.62420521, ...,  0.64259497,\n",
       "         0.9687384 ,  0.84432121],\n",
       "       [ 1.75486502, -0.57369368, -0.28401079, ...,  0.64259497,\n",
       "        -1.03227043,  0.32472465]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a7c581",
   "metadata": {},
   "source": [
    "# **Initialize & build the model**\n",
    "\n",
    "`INPUT = Number columns (Independet ) HIDDEN - AF HIDDEN -AF . . . N OUTPUT (1,2) -Sigmoid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0651b693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Initializing the ANN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6d4a815",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# The amount of nodes (dimensions) in hidden layer should be the average of input and output layers, in this case 6.\n",
    "# This adds the input layer (by specifying input dimension) AND the first hidden layer (units)\n",
    "classifier.add(Dense(activation = 'relu', input_dim = 11, units=256, kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f28bf2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the hidden layer\n",
    "classifier.add(Dense(activation = 'relu', units=512, kernel_initializer='uniform'))\n",
    "classifier.add(Dense(activation = 'relu', units=256, kernel_initializer='uniform'))\n",
    "classifier.add(Dense(activation = 'relu', units=128, kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20f61224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the output layer\n",
    "# Notice that we do not need to specify input dim. \n",
    "# we have an output of 1 node, which is the the desired dimensions of our output (stay with the bank or not)\n",
    "# We use the sigmoid because we want probability outcomes\n",
    "classifier.add(Dense(activation = 'sigmoid', units=1, kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e400373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimizer with default learning rate\n",
    "# sgd_optimizer = tf.keras.optimizers.SGD()\n",
    "# Compile the model\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd1afeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 256)               3072      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               131328    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 299009 (1.14 MB)\n",
      "Trainable params: 299009 (1.14 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "95de74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 2s 7ms/step - loss: 0.2700 - accuracy: 0.8830 - val_loss: 0.3942 - val_accuracy: 0.8565\n",
      "Epoch 2/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2617 - accuracy: 0.8907 - val_loss: 0.3813 - val_accuracy: 0.8605\n",
      "Epoch 3/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2517 - accuracy: 0.8924 - val_loss: 0.4118 - val_accuracy: 0.8535\n",
      "Epoch 4/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2536 - accuracy: 0.8909 - val_loss: 0.4178 - val_accuracy: 0.8625\n",
      "Epoch 5/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2401 - accuracy: 0.8941 - val_loss: 0.3981 - val_accuracy: 0.8505\n",
      "Epoch 6/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2346 - accuracy: 0.8956 - val_loss: 0.4594 - val_accuracy: 0.8530\n",
      "Epoch 7/20\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.2254 - accuracy: 0.9014 - val_loss: 0.4397 - val_accuracy: 0.8570\n",
      "Epoch 8/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2146 - accuracy: 0.9051 - val_loss: 0.4750 - val_accuracy: 0.8465\n",
      "Epoch 9/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2112 - accuracy: 0.9035 - val_loss: 0.4841 - val_accuracy: 0.8490\n",
      "Epoch 10/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2000 - accuracy: 0.9090 - val_loss: 0.5645 - val_accuracy: 0.8450\n",
      "Epoch 11/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.2001 - accuracy: 0.9064 - val_loss: 0.5328 - val_accuracy: 0.8440\n",
      "Epoch 12/20\n",
      "250/250 [==============================] - 2s 8ms/step - loss: 0.1851 - accuracy: 0.9145 - val_loss: 0.5550 - val_accuracy: 0.8590\n",
      "Epoch 13/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.1806 - accuracy: 0.9181 - val_loss: 0.5790 - val_accuracy: 0.8430\n",
      "Epoch 14/20\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.1684 - accuracy: 0.9237 - val_loss: 0.6381 - val_accuracy: 0.8470\n",
      "Epoch 15/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.1523 - accuracy: 0.9289 - val_loss: 0.7997 - val_accuracy: 0.8200\n",
      "Epoch 16/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.1682 - accuracy: 0.9240 - val_loss: 0.7215 - val_accuracy: 0.8340\n",
      "Epoch 17/20\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.1542 - accuracy: 0.9286 - val_loss: 0.7315 - val_accuracy: 0.8220\n",
      "Epoch 18/20\n",
      "250/250 [==============================] - 2s 7ms/step - loss: 0.1397 - accuracy: 0.9354 - val_loss: 0.8238 - val_accuracy: 0.8280\n",
      "Epoch 19/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.1433 - accuracy: 0.9342 - val_loss: 0.7892 - val_accuracy: 0.8165\n",
      "Epoch 20/20\n",
      "250/250 [==============================] - 2s 6ms/step - loss: 0.1549 - accuracy: 0.9333 - val_loss: 0.6714 - val_accuracy: 0.8220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x19b2a327110>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(\n",
    "    X_train, y_train,           \n",
    "    validation_data=(X_test,y_test),\n",
    "    epochs=20,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06863c04",
   "metadata": {},
   "source": [
    "**Predict the results using 0.5 as a threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a787c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.0385748e-01],\n",
       "       [1.5050180e-01],\n",
       "       [2.2405563e-02],\n",
       "       ...,\n",
       "       [2.7458313e-08],\n",
       "       [5.1914173e-01],\n",
       "       [5.2833658e-01]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85c6ef85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       ...,\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To use the confusion Matrix, we need to convert the probabilities that a customer will leave the bank into the form true or false. \n",
    "# So we will use the cutoff value 0.5 to indicate whether they are likely to exit or not.\n",
    "y_pred = (y_pred > 0.5)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78173e20",
   "metadata": {},
   "source": [
    "**Print the Accuracy score and confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68ec9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1427,  168],\n",
       "       [ 188,  217]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "\n",
    "cm1 = confusion_matrix(y_test, y_pred)\n",
    "cm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9f28fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.89      0.89      1595\n",
      "           1       0.56      0.54      0.55       405\n",
      "\n",
      "    accuracy                           0.82      2000\n",
      "   macro avg       0.72      0.72      0.72      2000\n",
      "weighted avg       0.82      0.82      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73468060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.2 % of testing data was classified correctly\n"
     ]
    }
   ],
   "source": [
    "accuracy_model1 = ((cm1[0][0]+cm1[1][1])*100)/(cm1[0][0]+cm1[1][1]+cm1[0][1]+cm1[1][0])\n",
    "print (accuracy_model1, '% of testing data was classified correctly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sure, let's break down the provided code step by step:\n",
    "\n",
    "# Library Imports:\n",
    "# python\n",
    "# Copy code\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "# from sklearn.svm import SVC, LinearSVC\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn import metrics\n",
    "# from sklearn import preprocessing\n",
    "# These libraries are imported for various purposes such as data manipulation, data visualization, machine learning, and preprocessing.\n",
    "# Loading the Dataset:\n",
    "# python\n",
    "# Copy code\n",
    "# df = pd.read_csv('churn_modelling.csv')\n",
    "# This line reads a CSV file named 'churn_modelling.csv' and loads its content into a DataFrame called 'df'. The DataFrame is used to organize and analyze the dataset.\n",
    "# Exploring the Dataset:\n",
    "# python\n",
    "# Copy code\n",
    "# df.info()\n",
    "# df.head()\n",
    "# df.info(): This method provides information about the DataFrame, including the number of columns, the data types of columns, and the count of non-null values.\n",
    "\n",
    "# df.head(): This method displays the first few rows of the DataFrame, allowing you to inspect the data.\n",
    "\n",
    "# Data Cleaning:\n",
    "# python\n",
    "# Copy code\n",
    "# df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)\n",
    "# This line drops the 'RowNumber', 'CustomerId', and 'Surname' columns from the DataFrame. These columns are removed as they are unlikely to be relevant for the analysis.\n",
    "# python\n",
    "# Copy code\n",
    "# df.isna().sum()\n",
    "# df.describe()\n",
    "# df.isna().sum(): This code checks for missing values (NaN) in each column and prints the count of missing values for each column.\n",
    "\n",
    "# df.describe(): The describe() method provides summary statistics for the numeric columns in the DataFrame, such as count, mean, standard deviation, and percentiles.\n",
    "\n",
    "# Separating Features and Labels:\n",
    "# python\n",
    "# Copy code\n",
    "# X = df.iloc[:, :df.shape[1] - 1].values  # Independent Variables\n",
    "# y = df.iloc[:, -1].values  # Dependent Variable\n",
    "# X.shape, y.shape\n",
    "# Here, the dataset is separated into two parts:\n",
    "# X: This contains the independent variables (features) and is obtained by selecting all columns except the last one from the DataFrame and converting it to a NumPy array.\n",
    "# y: This contains the dependent variable (labels) and is obtained by selecting the last column from the DataFrame and converting it to a NumPy array.\n",
    "# Encoding Categorical Data:\n",
    "# python\n",
    "# Copy code\n",
    "# label_X_country_encoder = LabelEncoder()\n",
    "# X[:, 1] = label_X_country_encoder.fit_transform(X[:, 1])\n",
    "\n",
    "# label_X_gender_encoder = LabelEncoder()\n",
    "# X[:, 2] = label_X_gender_encoder.fit_transform(X[:, 2])\n",
    "# Categorical data in columns 'Geography' (index 1) and 'Gender' (index 2) is encoded using Label Encoding. This transforms categorical values into numerical representations.\n",
    "# One-Hot Encoding for Categorical Data:\n",
    "# python\n",
    "# Copy code\n",
    "# transform = ColumnTransformer([(\"countries\", OneHotEncoder(), [1])], remainder=\"passthrough\") # 1 is the country column\n",
    "# X = transform.fit_transform(X)\n",
    "# X = X[:, 1:]\n",
    "# One-Hot Encoding is applied to the 'Geography' column (index 1) to create separate dimensions for different countries. The original column is replaced with multiple binary columns representing each country. The 'remainder=\"passthrough\"' parameter ensures that the other columns are kept in the dataset.\n",
    "# Dimensionality Reduction:\n",
    "# python\n",
    "# Copy code\n",
    "# X = X[:, 1:]\n",
    "# X.shape\n",
    "# After One-Hot Encoding, one of the binary columns for 'Geography' is removed to avoid multicollinearity. The shape of the dataset is updated.\n",
    "# Splitting the Dataset into Training and Test Sets:\n",
    "# python\n",
    "# Copy code\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "# The dataset is split into training and test sets using the train_test_split function. The split ratio is 80% for training and 20% for testing, and a random state is set for reproducibility.\n",
    "# Data Normalization (Standardization):\n",
    "# python\n",
    "# Copy code\n",
    "# sc = StandardScaler()\n",
    "# X_train = sc.fit_transform(X_train)\n",
    "# X_test = sc.transform(X_test)\n",
    "# The features are standardized using the StandardScaler to have a mean of 0 and a standard deviation of 1, making them suitable for training machine learning models.\n",
    "# Initializing and Building the Neural Network Model:\n",
    "# python\n",
    "# Copy code\n",
    "# from tensorflow.keras.models import Sequential\n",
    "\n",
    "# # Initializing the ANN\n",
    "# classifier = Sequential()\n",
    "# from tensorflow.keras.layers import Dense\n",
    "\n",
    "# # Building the neural network with multiple layers (input, hidden, and output).\n",
    "# A neural network model is initialized using Keras, a high-level deep learning library. It's a feedforward neural network with layers specified.\n",
    "# Model Summary:\n",
    "# python\n",
    "# Copy code\n",
    "# classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# classifier.summary()\n",
    "# The model is compiled with the Adam optimizer, binary cross-entropy loss function (suitable for binary classification), and accuracy as the evaluation metric. The model summary provides an overview of the architecture.\n",
    "# Model Training:\n",
    "# python\n",
    "# Copy code\n",
    "# classifier.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=20, batch_size=32)\n",
    "# The model is trained on the training data with 20 epochs and a batch size of 32.\n",
    "# Making Predictions:\n",
    "# python\n",
    "# Copy code\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# y_pred = (y_pred > 0.5)\n",
    "# Predictions are made on the test data. A threshold of 0.5 is applied to classify customers as likely to exit (True) or not (False).\n",
    "# Model Evaluation:\n",
    "# python\n",
    "# Copy code\n",
    "# from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# cm1 = confusion_matrix(y_test, y_pred)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "# accuracy_model1 = ((cm1[0][0] + cm1[1][1]) * 100) / (cm1[0][0] + cm1[1][1] + cm1[0][1] + cm1[1][0])\n",
    "# The confusion matrix and classification report are generated to evaluate the model's performance. The accuracy is also calculated as the percentage of correctly classified instances."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
